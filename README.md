# rml-normalizer

In order to run the whole application including data generator, RML-Normalizer and experiments, one need to call the following command:

- `python3 app/run_all.py`

This command executes subsequently the following different components:

- Data generator
- RML-normalizer
- Experiments

As Input, one need to feed the following directories with their corresponding input files. Following table shows just the directories along with related input files used in exeriments done in the master thesis of *"Normalization Techniques For Improving The Perfromance Of Knowledge Graph Creation Pipelines"*:

| Directory                               | Input files             |description|
|---------------------------------------  |-------------------------|--------------------------------------------------------------------------|
| app/data_generator/source/generator/    |app/input/data/synthetic/|Configurations for data generation (in case of generating synthetic data) |
| app/source/data/                	  |app/input/data/real/	    |Original CSV data source files (in case of using real data)              |
| app/source/fd/          		  |app/input/fd/	    |List of functional dependencies holding in the data source, whether real or synthetic.|
| app/source/mappings/                 	  |app/input/mappings/	    |Original RML mapping rules defined over the data sources                              |

## Data generator

To generate data using the testbed generator, it is necessary create a new file consiting of the FD's structures and consequently to update the configfile existing in app/data_generator/source/generator/configfile.ini to refer to the newly created file. Structure of this file and also the configuration file needs to be as follows:

### app/data_generator/source/generator/data.ini
```ini
[level n]
# True if this level (attribute) implies all other previous levels (sttributes), and False if there is no dependencies.
is_parent = False
# Number of values in the current level's domain (domain of attribute)
number_of_distinct_vals = 2
# This is calculated automatically by the tool
total_vals = 16
# The number of values in a domain which needs to be duplicated
number_of_dup_values = 2
```
### app/data_generator/source/generator/configfile.ini
```ini
[datasets]
# Number of dataset to be generated by this tool
number_of_datasets: 1

[dataset n]
# The path to the configuration file corresponding to structure of functional dependencies
tree_file: ./app/data_generator/source/generator/data.ini
# Number of attributes within the dataset to be generated
number_of_levels=6
```

## RML-normalizer

To run the normalization tool, one needs to provide the tool with CSV data file, a FD list (a turtle file including the sturcture) and a RML mapping rule over the CSV data file exisiting in app/source/data/, app/source/fd/ and app/source/mappings/ respectively. In order to run RML-Normalizer, it is necessary to create a functional dependecny list (possibly a closure of FD set) with the following structure:

### app/source/fd/fd.ttl

```turtle
@prefix fd: <http://semweb.mmlab.be/ns/rml#> .

<#Example> fd:key [fd:column_name "{A}";
                   fd:determine [fd:column_name "B";
                                 fd:dependant "{A}";
                                ];
                   fd:determine [fd:column_name "C";
                                 fd:dependant "{B}";
                                ];
                   fd:determine [fd:column_name "D";
                                 fd:dependant "{C}";
                                ];
                   fd:determine [fd:column_name "E";
                                 fd:dependant "{C}"
                                ];
                   fd:determine [fd:column_name "F";
                                 fd:dependant "{C}"
                                ]
].
```
																				
																				
Based on the above, the key, i.e., A, is implying/determining each and every attribute in this set of attributes and every other dependencies is shown by fd:dependant predicate. The fd:dependant contains the left side of a functional dependency as its object. This can be an attribute or a set of attributes.

In addition the configuration file configfile.ini in the directory app/source/ needs to be updated.

### app/source/configfile.ini
```ini
[default]
# Number of rules to be normalized
number_of_rules = 1
# Names of datasets to be normalized (possibly more than one, in case of running rules over different datasets but same fd list)
name_of_datasets = data.csv
# Path to the functional dependency list holding in a dataset(s)
fd = ./app/source/fd/fd.ttl

[dataset n]
# Path to the mapping rule to be normalized
mapping = ./app/source/mappings/mapping.ttl
```

Note that, in case of using testbed generator, the property name_of_datasets is updated automatically.

## Experiments

Finally, to run experiments, there is a configuration file for reading the data source files as well as mapping rules. This is located under the directory app/experiments/. In the case of running normalization prior to experiments (one can have the original data sources and mapping rules as well the normalized ones prepared and just start the experiments), every property in this file are updated automaticaly.

### app/experiments/configfile.ini

```ini
[rules]
# Number of rules and sources to be translated into RDF graphs
number_of_rules = 2

[rule n]
# Name of the mapping rule to be executed
filename = mapping--data_normal.ttl
# Name of the dataset(s) (possibly several datasets in case of normalized version) refered in the mapping rule
dataset_name = mapping--data-PT_C_E.csv,mapping--data-PT_C_D.csv,mapping--data-CT_B.csv
```

The output of experiments as well as all other previous tools is located in the app/experiments/graph/ which contains log files, RDF graph (generated by each differnt engine) and statistics of each steps (like testbed generation times, normalization times and experimental metrics)
